{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e46001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikingjelly.activation_based.monitor import OutputMonitor\n",
    "from spikingjelly.activation_based.layer import SynapseFilter\n",
    "from spikingjelly.activation_based.neuron import LIFNode\n",
    "from spikingjelly.activation_based.encoding import LatencyEncoder\n",
    "from jaxtyping import Float, Int64\n",
    "from torch import Tensor\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import numpy as np\n",
    "import spikingjelly.activation_based as snn\n",
    "from spikingjelly.activation_based import surrogate, neuron, functional\n",
    "from typing import Callable, Any, overload\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils.module import StochasticRound, JeffressLinear, TransposeLayer\n",
    "from utils.model import CCN\n",
    "from utils.datasets import generate_lp_dataset, generate_cosine_dataset, generate_1d_dot_classification_dataset, encode_temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15e0093",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = dict(\n",
    "    NUM_SAMPLES = 10000,  # 총 10000 개의 샘플 생성\n",
    "    NUM_EPOCHS = 200,\n",
    "    VECTOR_DIM = 3,      # 각 벡터는 2차원\n",
    "    MAX_VAL = 10.0,\n",
    "    TIME_STEPS = 17,     # SNN을 17 타임스텝 동안 실행\n",
    "    BATCH_SIZE = 32,\n",
    "    NUM_CLASSES = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f05c7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "np.random.seed(42)\n",
    "rng = torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b51320",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 50\n",
    "in_spikes = (torch.rand(size=[T]) >= 0.95).float()\n",
    "lp_syn = SynapseFilter(tau=1+1)\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.bar(torch.arange(0, T).tolist(), in_spikes, label='in spike')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('spike')\n",
    "plt.legend()\n",
    "\n",
    "out_i = []\n",
    "for i in range(T):\n",
    "    out_i.append(lp_syn(in_spikes[i]))\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(out_i, label='out i')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('i')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9306340b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = cfg[\"NUM_SAMPLES\"]  # 총 1000 개의 샘플 생성\n",
    "NUM_EPOCHS = cfg[\"NUM_EPOCHS\"]\n",
    "VECTOR_DIM = cfg[\"VECTOR_DIM\"]      # 각 벡터는 3차원\n",
    "MAX_VAL = cfg[\"MAX_VAL\"]\n",
    "TIME_STEPS = cfg[\"TIME_STEPS\"]     # SNN을 16 타임스텝 동안 실행\n",
    "BATCH_SIZE = cfg[\"BATCH_SIZE\"]\n",
    "NUM_CLASSES = cfg[\"NUM_CLASSES\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fa9483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.arange(0, TIME_STEPS, 1)\n",
    "# y = x.flip(0)\n",
    "# X = torch.stack([x, y], dim=-1)\n",
    "# X = torch.nn.functional.one_hot(X, num_classes=TIME_STEPS)\n",
    "# X = X.permute(-1, 0, 1)[:, None, ...]\n",
    "# print(X.shape)\n",
    "# for i in range(X.shape[-2]):\n",
    "#     print(X[:,0,i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789327e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = SDCLinear(TIME_STEPS, 2)\n",
    "# print(\"X.shape:\", X.shape)\n",
    "# Y = l(X)\n",
    "# print(\"Y.shape:\", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818b2c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(threshold=10_000, precision=2, linewidth=160, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891ee9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c in range(TIME_STEPS):\n",
    "#     print(Y[:,0,c,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f977a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.nn.Sequential(\n",
    "#     SDCLinear(out_features=1).to(device),\n",
    "#     LIFNode()\n",
    "# )\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532de283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_data, y_data = generate_1d_dot_classification_dataset(NUM_SAMPLES, num_classes=NUM_CLASSES) # N 2, N 1\n",
    "# X_data = torch.stack([torch.FloatTensor(encode_temporal(X_data[:,0], TIME_STEPS)),\n",
    "#                       torch.FloatTensor(encode_temporal(X_data[:,1], TIME_STEPS))],\n",
    "#                      dim=2) # T N 2\n",
    "# y_data = torch.FloatTensor(y_data) / y_data.max() # N 1\n",
    "# dataset = torch.utils.data.TensorDataset(X_data.transpose(1, 0), y_data)  # T N 2 -> N T 2\n",
    "\n",
    "# train_set, test_set = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     train_set,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     shuffle=True,\n",
    "#     drop_last=False\n",
    "#     )\n",
    "# test_loader = torch.utils.data.DataLoader(\n",
    "#     test_set,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     shuffle=False,\n",
    "#     drop_last=False\n",
    "# )\n",
    "\n",
    "# criterion = temporal_efficient_training_cross_entropy\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=120 * (NUM_SAMPLES // BATCH_SIZE))\n",
    "# loss = torch.tensor(float(\"inf\"))\n",
    "\n",
    "# pbar = tqdm(range(120))\n",
    "# pred_hist, target_hist, err_hist = [], [], []\n",
    "# # wandb_table = wandb.Table(columns=[f\"v_{t}\" for t in range(TIME_STEPS)], log_mode=\"INCREMENTAL\")\n",
    "# with wandb.init(project=\"DelayedTemporal\",\n",
    "#                 config=cfg) as run:\n",
    "#     for epoch in pbar:\n",
    "#         model.train()\n",
    "#         for i, batch in enumerate(tqdm(train_loader, leave=False)):\n",
    "#             inputs:Float[Tensor, \"N T 2 D\"]; targets:Float[Tensor, \"N D\"]\n",
    "#             inputs, targets = batch\n",
    "#             inputs = inputs.to(device); targets = targets.to(device)\n",
    "#             # 모델 학습 코드 추가\n",
    "#             inputs = inputs.transpose(1, 0) # N T 2 -> T N 2\n",
    "#             inputs = inputs[:,:,None,:] # T N 2 -> T N 1 2\n",
    "#             out = model(inputs) #  -> model -> N D\n",
    "#             loss = criterion(out, targets.long()) #+ model.loss\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             pbar.set_postfix({\"loss\": loss.item(), \"scale\":(out.max()-out.min()).item()})\n",
    "#             if i % 10 == 0:\n",
    "#                 # wandb_table.add_data(*v_seq.mean(dim=(1,2,3)).tolist())\n",
    "#                 run.log({\"loss\": loss.item(),\n",
    "#                         \"err\":(out - targets).abs().mean().item()}\n",
    "#                         |\n",
    "#                         {f\"delay_{j}\": model[0]._delay[j] for j in range(model[0]._delay.shape[0])})\n",
    "#         scheduler.step()\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             model.eval()\n",
    "#             for batch in tqdm(test_loader, leave=False):\n",
    "#                 inputs, targets = batch\n",
    "#                 inputs = inputs.to(device); targets = targets.to(device)\n",
    "#                 # 모델 학습 코드 추가\n",
    "#                 out = model(inputs.transpose(1, 0)) # NT(2D)->TN(2D)->model->N\n",
    "                \n",
    "#                 loss = criterion(out, targets)\n",
    "#                 pred_hist.extend(out.squeeze().tolist())\n",
    "#                 target_hist.extend(targets.squeeze().tolist())\n",
    "#                 err_hist.extend((out.squeeze() - targets.squeeze()).abs().tolist())\n",
    "#                 pbar.set_postfix({\"loss\": loss.item(), \"pred\": pred_hist[-1], \"target\": target_hist[-1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0963381",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CCN(vector_dim = VECTOR_DIM, cc_acc=TIME_STEPS-1, feature_dims=[]).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783028a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data, y_data = generate_lp_dataset(NUM_SAMPLES, VECTOR_DIM, max_val=MAX_VAL)\n",
    "X_data = torch.stack([torch.FloatTensor(encode_temporal(X_data[:,0,:], TIME_STEPS, time_norm=True)),\n",
    "                      torch.FloatTensor(encode_temporal(X_data[:,1,:], TIME_STEPS, time_norm=True))],\n",
    "                     dim=2) # T N 2D\n",
    "y_data = torch.FloatTensor(y_data) / y_data.max() # N D\n",
    "dataset = torch.utils.data.TensorDataset(X_data.transpose(1, 0), y_data)  # T N 2D -> N T 2D\n",
    "\n",
    "train_set, test_set = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=False\n",
    "    )\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS * (NUM_SAMPLES // BATCH_SIZE))\n",
    "loss = torch.tensor(float(\"inf\"))\n",
    "\n",
    "pbar = tqdm(range(NUM_EPOCHS))\n",
    "pred_hist, target_hist, err_hist = [], [], []\n",
    "# wandb_table = wandb.Table(columns=[f\"v_{t}\" for t in range(TIME_STEPS)], log_mode=\"INCREMENTAL\")\n",
    "train_step = 0\n",
    "eval_step = 0\n",
    "substep = 0\n",
    "with wandb.init(project=\"DelayedTemporal\",\n",
    "                config=cfg) as run:\n",
    "    run.define_metric(\"train/*\", step_metric=\"train_step\")\n",
    "    run.define_metric(\"delay/*\", step_metric=\"train_step\")\n",
    "    run.define_metric(\"SDC/*\", step_metric=\"train_step\")\n",
    "    run.define_metric(\"Mem/*\", step_metric=\"substep\")\n",
    "    run.define_metric(\"Spike/*\", step_metric=\"substep\")\n",
    "    run.define_metric(\"eval/*\", step_metric=\"eval_step\")\n",
    "    for epoch in pbar:\n",
    "        model.train()\n",
    "        for i, batch in enumerate(tqdm(train_loader, leave=False)):\n",
    "            inputs:Float[Tensor, \"N T 2 D\"]; targets:Float[Tensor, \"N D\"]\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device); targets = targets.to(device)\n",
    "            v_seq_pt = [] # list to save membrane potential sequences\n",
    "            out = model(inputs.transpose(1, 0), v_seq_pt=v_seq_pt) # N T 2 D -> T N 2 D -> model -> N D\n",
    "            loss = criterion(out, targets) #+ model.loss\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # print(model.model[1].delay.grad)\n",
    "            optimizer.step()\n",
    "            \n",
    "            v_seq = v_seq_pt[-1]\n",
    "            pbar.set_postfix({\"loss\": loss.item(), \"scale\":(out.max()-out.min()).item()})\n",
    "            if i % 10 == 0:\n",
    "                # wandb_table.add_data(*v_seq.mean(dim=(1,2,3)).tolist())\n",
    "                run.log({\n",
    "                    \"train_step\": (train_step := train_step + 1),\n",
    "                    \"train/loss\": loss.item(),\n",
    "                    \"train/err\":(out - targets).abs().mean().item(),\n",
    "                    }|{\n",
    "                        f\"delay/{j}\": model.model[1].delay[j, 0] for j in range(model.model[1].delay.shape[0])\n",
    "                        # }|{\n",
    "                        #     f\"SDC/rate_{i}\": model.stats['model.2'][-1][:,0,0,i].mean() for i in range(v_seq.shape[-1])\n",
    "                            })\n",
    "                for t in range(TIME_STEPS):\n",
    "                    run.log({\n",
    "                        \"substep\": (substep := substep + 1),\n",
    "                    }|{\n",
    "                        f\"Mem/mem_{i}\": v_seq[t,0,0,i] for i in range(v_seq.shape[-1])\n",
    "                    }|{\n",
    "                        f\"Spike/rate_{i}\": model.stats['model.2'][-1][t,0,0,i] for i in range(v_seq.shape[-1])\n",
    "                    })\n",
    "        scheduler.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for batch in tqdm(test_loader, leave=False):\n",
    "                inputs, targets = batch\n",
    "                inputs = inputs.to(device); targets = targets.to(device)\n",
    "                out = model(inputs.transpose(1, 0)) # NT(2D)->TN(2D)->model->N\n",
    "                \n",
    "                loss = criterion(out, targets)\n",
    "                pred_hist.extend(out.squeeze().tolist())\n",
    "                target_hist.extend(targets.squeeze().tolist())\n",
    "                err_hist.extend((out.squeeze() - targets.squeeze()).abs().tolist())\n",
    "                pbar.set_postfix({\"loss\": loss.item(), \"pred\": pred_hist[-1], \"target\": target_hist[-1]})\n",
    "                \n",
    "                run.log({\"eval_step\": (eval_step := eval_step + 1),\n",
    "                         \"eval/loss\": loss.item(),\n",
    "                         \"eval/err\":  (out - targets).abs().mean().item()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c56575",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(err_hist, linewidth=0.03, label=\"Difference\")\n",
    "plt.ylim(-1, 1)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110ba338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c533125",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(1000):\n",
    "    print(abs(np.array(pred_hist)[-n] - np.array(target_hist)[-n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2816ef33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
